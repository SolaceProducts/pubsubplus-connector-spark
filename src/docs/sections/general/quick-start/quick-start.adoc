:doctype: book
:toc: preamble
:toclevels: 3
:icons: font


This quick start shows how to deploy connector in Databricks Cluster.

Solace Spark Connector is available in maven central with below coordinates.

[source,xml,subs="+attributes"]
----
<dependency>
    <groupId>com.solacecoe.connectors</groupId>
    <artifactId>pubsubplus-connector-spark</artifactId>
    <version>{version}</version>
</dependency>
----

Steps to deploy the connector

. Create a Databricks Cluster with Spark Version 3.5.1 and Scala 2.12
. Once the cluster is started choose libraries and select maven as connector source. Enter above maven coordinates to download connector from maven central.

NOTE: Before installing latest version of connector make sure earlier versions of solace spark connector are completely deleted from cluster. This is to ensure there are no version conflicts to start the connector.

. Create new scala notebook and copy below code snippets.

. Creating Solace Source Streaming Queries

.. First, let's create a Solace consumer to read messages and write to parquet file. Update configuration options as per your environment.

... Scala Code Snippet
+
[source,scala]
----
val spark = SparkSession.builder.appName("SolaceSparkStreaming").getOrCreate()
val streamName = "solace-spark-connector-sample-test"
val df = spark.readStream.format("solace")
    .option("host", "tcp://localhost:55555")
    .option("vpn", "default")
    .option("username", "default")
    .option("password", "default")
    .option("queue", "<queue-name>")
    .option("connectRetries", 2)
    .option("reconnectRetries", 2)
    .option("batchSize", 100) // <5>
    .option("includeHeaders", true)
    .option("partitions", 1)
    .load()

// write to parquet file
val query = df.writeStream
    .format("parquet")
    .outputMode("append")
    .queryName(streamName)
    .option("checkpointLocation", s"/spark_checkpoints/solace_spark_connector_checkpoint/$streamName/")
    .option("path", s"/solace_spark_connector_parquet_output/$streamName/")
    .start()

query.awaitTermination()
----

... Python Code Snippet
+
[source, python]
----
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SolaceSparkStreaming") \
    .getOrCreate()

stream_name = "solace-spark-connector-sample-test"

df = spark.readStream \
    .format("solace") \
    .option("host", "tcp://localhost:55555") \
    .option("vpn", "default") \
    .option("username", "default") \
    .option("password", "default") \
    .option("queue", "<queue-name>") \
    .option("connectRetries", 2) \
    .option("reconnectRetries", 2) \
    .option("batchSize", 100) \
    .option("includeHeaders", "true") \
    .option("partitions", 1) \
    .load()

query = df.writeStream \
    .format("parquet") \
    .outputMode("append") \
    .queryName(stream_name) \
    .option("checkpointLocation", f"/spark_checkpoints/solace_spark_connector_checkpoint/{stream_name}/") \
    .option("path", f"/solace_spark_connector_parquet_output/{stream_name}/") \
    .start()

query.awaitTermination()
----

... Java Code Snippet
+
[source, java]
----
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;

public class SolaceSparkStreamingJava {
    public static void main(String[] args) throws StreamingQueryException {
        SparkSession spark = SparkSession.builder()
                .appName("SolaceSparkStreaming")
                .getOrCreate();

        String streamName = "solace-spark-connector-sample-test";

        Dataset<Row> df = spark.readStream()
                .format("solace")
                .option("host", "tcp://localhost:55555")
                .option("vpn", "default")
                .option("username", "default")
                .option("password", "default")
                .option("queue", "<queue-name>")
                .option("connectRetries", 2)
                .option("reconnectRetries", 2)
                .option("batchSize", 100)
                .option("includeHeaders", true)
                .option("partitions", 1)
                .load();

        StreamingQuery query = df.writeStream()
                .format("parquet")
                .outputMode("append")
                .queryName(streamName)
                .option("checkpointLocation", "/spark_checkpoints/solace_spark_connector_checkpoint/" + streamName + "/")
                .option("path", "/solace_spark_connector_parquet_output/" + streamName + "/")
                .start();

        query.awaitTermination();
    }
}
----

TIP: For optimal throughput, configure the Solace queue's 'Maximum Delivered Unacknowledged Messages per Flow' property to a value equal to twice the batch size.

.. Finally, let's read data from the parquet file from the location configured above

... Scala Code Snippet
+
[source,scala]
----
val streamName = "solace-spark-connector-sample-test" // this should be equal to stream name variable provided as above
val df = spark.read.format("parquet").load(s"/solace_spark_connector_parquet_output/$streamName/") // this should be same as value of "path" property configured in write stream as above
display(df)
// Ex: Parse payload as string
df.select($"payload".cast("STRING"))
----

... Python Code Snippet
+
[source, python]
----
from pyspark.sql.functions import col

stream_name = "solace-spark-connector-sample-test"

df = spark.read \
    .format("parquet") \
    .load(f"/solace_spark_connector_parquet_output/{stream_name}/")

# If using Databricks or notebook environment that supports display()
display(df)

# Parse 'payload' column as STRING
df_parsed = df.select(col("payload").cast("string"))

# Show the parsed payload
df_parsed.show()
----

... Java Code Snippet
+
[source, java]
----
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import static org.apache.spark.sql.functions.col;

public class ReadParquetAndParsePayload {
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName("SolaceReadParquet")
                .getOrCreate();

        String streamName = "solace-spark-connector-sample-test";

        Dataset<Row> df = spark.read()
                .format("parquet")
                .load("/solace_spark_connector_parquet_output/" + streamName + "/");

        df.show();  // Equivalent of display()

        Dataset<Row> dfParsed = df.select(col("payload").cast("string"));

        dfParsed.show();  // Show the casted payload
    }
}
----

. Creating Solace Sink Streaming Queries

.. First, let's create a parquet consumer to read messages and publish to Solace. Update configuration options as per your environment.
.. Next, create a queue on Solace Broker and subscribe to the topic published by Solace Spark Connector. In this case it is ``solace/spark/publish``

... Scala Code Snippet
+
[source,scala]
----
val streamName = "solace-spark-connector-sample-test"
val parquetData = spark.read.parquet("<path-to-parquet-file>")
val struct_stream = spark.readStream
.schema(parquetData.schema)
.parquet("<path-to-parquet-file>")

// write to parquet file
val query = df.writeStream
      .format("solace")
      .option("host", "tcp://localhost:55555")
      .option("vpn", "default")
      .option("username", "default")
      .option("password", "default")
      .option("topic", "solace/spark/stream/processing/result") // This can be commented if topic column is present in output dataframe.
      .option("id", "<application-message-id>")
      .outputMode("append")
      .option("checkpointLocation", "/spark_checkpoints/solace_spark_connector_checkpoint/" + streamName + "/")
      .start()

query.awaitTermination()
----

... Python Code Snippet
+
[source, python]
----
from pyspark.sql import SparkSession

stream_name = "solace-spark-connector-sample-test"
path_to_parquet = "<path-to-parquet-file>"

spark = SparkSession.builder.appName("SolaceSparkStreaming").getOrCreate()

# Read static data to infer schema
parquet_data = spark.read.parquet(path_to_parquet)

# Read the streaming source
struct_stream = spark.readStream.schema(parquet_data.schema).parquet(path_to_parquet)

# Write to Solace
query = struct_stream.writeStream \
    .format("solace") \
    .option("host", "tcp://localhost:55555") \
    .option("vpn", "default") \
    .option("username", "default") \
    .option("password", "default") \
    .option("topic", "solace/spark/stream/processing/result") \ # This can be commented if topic column is present in output dataframe.
    .option("id", "<application-message-id>") \ # This can be commented if Id column is present in output dataframe.
    .outputMode("append") \
    .option("checkpointLocation", f"/spark_checkpoints/solace_spark_connector_checkpoint/{stream_name}/") \
    .start()

query.awaitTermination()

----

... Java Code Snippet
+
[source, java]
----
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.streaming.StreamingQuery;

public class SolaceSparkStreaming {
    public static void main(String[] args) throws Exception {
        String streamName = "solace-spark-connector-sample-test";
        String parquetPath = "<path-to-parquet-file>";

        SparkSession spark = SparkSession.builder()
                .appName("SolaceSparkStreaming")
                .getOrCreate();

        // Read static schema
        Dataset<Row> parquetData = spark.read().parquet(parquetPath);

        // Read stream using inferred schema
        Dataset<Row> structStream = spark.readStream()
                .schema(parquetData.schema())
                .parquet(parquetPath);

        // Write to Solace
        StreamingQuery query = structStream.writeStream()
                .format("solace")
                .option("host", "tcp://localhost:55555")
                .option("vpn", "default")
                .option("username", "default")
                .option("password", "default")
                .option("topic", "solace/spark/stream/processing/result") // This can be commented if topic column is present in output dataframe.
                .option("id", "<application-message-id>") // This can be commented if Id column is present in output dataframe.
                .outputMode("append")
                .option("checkpointLocation", "/spark_checkpoints/solace_spark_connector_checkpoint/" + streamName + "/")
                .start();

        query.awaitTermination();
    }
}
----