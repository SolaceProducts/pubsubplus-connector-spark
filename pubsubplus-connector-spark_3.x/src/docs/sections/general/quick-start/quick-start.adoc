:doctype: book
:toc: preamble
:toclevels: 3
:icons: font


This quick start shows how to deploy connector in Databricks Cluster.

Solace Spark Connector is available in maven central with below coordinates.

[source,xml,subs="+attributes"]
----
<dependency>
    <groupId>com.solacecoe.connectors</groupId>
    <artifactId>pubsubplus-connector-spark</artifactId>
    <version>{version}</version>
</dependency>
----

Steps to deploy the connector

. Create a Databricks Cluster with Spark Version 3.5.1 and Scala 2.12
. Once the cluster is started choose libraries and select maven as connector source. Enter above maven coordinates to download connector from maven central.

NOTE: Before installing latest version of connector make sure earlier versions of solace spark connector are completely deleted from cluster. This is to ensure there are no version conflicts to start the connector.

. Create new scala notebook and copy below code snippets.

. Creating Solace Source Streaming Queries

.. First, let's create a Solace consumer to read messages and write to parquet file. Update configuration options as per your environment.
+
[source,scala]
----
    val spark = SparkSession.builder.appName("SolaceSparkStreaming").getOrCreate()
    val streamName = "solace-spark-connector-sample-test"
    val df = spark.readStream.format("solace")
        .option("host", "tcp://localhost:55555")
        .option("vpn", "default")
        .option("username", "default")
        .option("password", "default")
        .option("queue", "<queue-name>")
        .option("connectRetries", 2)
        .option("reconnectRetries", 2)
        .option("batchSize", 100)
        .option("includeHeaders", true)
        .option("partitions", 1)
        .load()

    // write to parquet file
    val query = df.writeStream
    .format("parquet")
    .outputMode("append")
    .queryName(streamName)
    .option("checkpointLocation", s"/spark_checkpoints/solace_spark_connector_checkpoint/$streamName/")
    .option("path", s"/solace_spark_connector_parquet_output/$streamName/")
    .start()

    query.awaitTermination()
----
.. Finally, let's read data from the parquet file from the location configured above
+
[source,scala]
----
    val streamName = "solace-spark-connector-sample-test" // this should be equal to stream name variable provided as above
    val df = spark.read.format("parquet").load(s"/solace_spark_connector_parquet_output/$streamName/") // this should be same as value of "path" property configured in write stream as above
    display(df)
    // Ex: Parse payload as string
    df.select($"payload".cast("STRING"))
----

. Creating Solace Sink Streaming Queries

.. First, let's create a parquet consumer to read messages and publish to Solace. Update configuration options as per your environment.
+
[source,scala]
----
    val parquetData = spark.read.parquet("<path-to-parquet-file>")
    val struct_stream = spark.readStream
    .schema(parquetData.schema)
    .parquet("<path-to-parquet-file>")

    // write to parquet file
    val query = df.writeStream
    .foreachBatch((outputDf: Dataset[org.apache.spark.sql.Row], bid: Long) => {
         // Process valid data frames only
         if (!outputDf.isEmpty) {
            // business logic
            try {
                outputDf.show()
                outputDf.write.format("solace")
                  .option("host", "tcp://localhost:55555")
                  .option("vpn", "default")
                  .option("username", "default")
                  .option("password", "default")
                  .option("batchSize", outputDf.count())
                  .option("topic", "solace/spark/publish") // This can be commented if topic column is present in output dataframe.
                  .option("id", "<application-message-id>") // This can be commented if Id column is present in output dataframe.
                  .mode(SaveMode.Append)
                  .save()
            } catch {
              case e: Exception => {
                // Any logic to handle failed messages. Connector will log application message id along with exception as json object.
                throw new RuntimeException(e)
              }
            }
         }
    }).start()

    query.awaitTermination()
----
.. Finally, create a queue on Solace Broker and subscribe to the topic to spool the messages published by Solace Spark Connector.