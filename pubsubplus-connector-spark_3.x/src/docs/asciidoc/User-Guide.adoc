= {connector-name}: User Guide
:doctype: book
:toc: preamble
:toclevels: 3
:icons: font
:imagesdir: {docdir}/../images
:version: {revnumber}
// Import common attributes
// include::{docdir}/../snippets/attributes/common.adoc[]
// Custom attributes
:overview: <product-overview>

// Github-Specific Settings
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

[preface]
== Preface

{overview}

== Getting Started

This guide assumes you are familiar with Spark set up and Spark Structured Streaming concepts. In the following sections we will show how to set up Solace Spark Connector to stream data from Solace to Spark.

=== Prerequisites

* https://solace.com/products/event-broker/[Solace PubSub+ Event Broker]
* Apache Spark 3.5.1, Scala 2.12

=== Quick Start common steps

include::{docdir}/../sections/general/quick-start/quick-start.adoc[leveloffset=+2]

NOTE: Above sample code used parquet as example data source. You can configure your required data source to write data.

=== Databricks Considerations

In case if you are using Shared compute cluster, make sure your cluster has https://docs.databricks.com/en/data-governance/unity-catalog/manage-privileges/allowlist.html[appropriate permissions] to install connector from maven central and access the jars. Please contact your Databricks administrator for required permissions.

=== Checkpointing & Acknowledgement

Solace Spark connector relies on Spark Checkpointing mechanism to resume from last successful message. Connector acknowledges & writes processed message id's to spark checkpoint whenever Spark sends commit signal to connector. Commit signal is an indication by Spark that data has been processed successfully.

=== Checkpoint Handling

In some cases, there might be checkpoint failures as spark may fail to write to checkpoint during instance crash or unavailability or other reasons. Though the connector will handle duplicates in most cases, we recommend to keep your downstream systems idempotent.

=== User Authentication

Solace Spark Connector supports Basic and OAuth authentication to Solace. Client Credentials flow is supported when connecting using OAuth.

If OAuth server is available use below options to fetch access token from endpoint. For property description please refer to <<Configuration>> section.

[source,scala]
----
    spark.readStream.format("solace").option("host", "")
    .option("vpn", "default")
    .option("solace.apiProperties.AUTHENTICATION_SCHEME", "AUTHENTICATION_SCHEME_OAUTH2")
    .option("solace.oauth.client.auth-server-url", "")
    .option("solace.oauth.client.client-id", "")
    .option("solace.oauth.client.credentials.client-secret", "")
    .option("solace.oauth.client.auth-server.client-certificate.file", "")
    .option("solace.oauth.client.auth-server.truststore.file", "")
    .option("solace.oauth.client.auth-server.truststore.password", "")
    .option("solace.oauth.client.auth-server.ssl.validate-certificate", false)
    .option("solace.oauth.client.token.refresh.interval", 110)
----

If rotating access token is present in file accessible by connector use below options to enable OAuth authentication to read access token from file. For property description please refer to <<solace-oAuth-client-refresh-interval, Solace OAuth Client Refresh Interval>> property. In case when access token file is not updated, connector retries the connection based on <<reconnect-retries, Reconnect Retries>> and stops if authentication is not successful.

[source,scala]
----
    spark.readStream.format("solace").option("host", "")
    .option("vpn", "default")
    .option("solace.apiProperties.AUTHENTICATION_SCHEME", "AUTHENTICATION_SCHEME_OAUTH2")
    .option("solace.oauth.client.access-token", "<absolute-path-to-token-file>")
    .option("solace.oauth.client.token.refresh.interval", 110)
----

NOTE: When access token is read from file, it may lose some of it's expiry time by the time it is accessed by connector. It is recommended to have minimal time difference between writing to file and access by the connector so that a valid new token is updated in solace session before expiry of old token.

=== Message Replay

Solace Spark Connector can replay messages using Solace Replay Log. Connector can replay all messages or after specific replication group message id or after specific timestamp. Please refer to https://docs.solace.com/Features/Replay/Msg-Replay-Concepts-Config.htm[Message Replay Configuration] to enable replay log in Solace PubSub+ broker.

=== Solace Spark Streaming Schema Structure

Solace Spark Connector transforms the incoming message to Spark row with below schema definition.

[cols="2m,2m", options="header"]
|===
| Column Name
| Column Type

| Id
| String

| Payload
| Binary

| PartitionKey
| String

| Topic
| String

| TimeStamp
| Timestamp

| Headers
| Map<string, binary>
|===

== Configuration

include::{docdir}/../sections/general/configuration/spark-config.adoc[leveloffset=+2]

NOTE: This connector is tested on Databricks environment with Cluster Version 14.3 LTS (includes Apache Spark 3.5.0, Scala 2.12)

== License

This project is licensed under the Solace Community License, Version 1.0. - See the `LICENSE` file for details.

include::{docdir}/../sections/general/support.adoc[leveloffset=+2]