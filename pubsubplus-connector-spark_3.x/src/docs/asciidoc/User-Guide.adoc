= {connector-name}: User Guide
:doctype: book
:toc: preamble
:toclevels: 3
:icons: font
:imagesdir: {docdir}/../images
:version: {revnumber}
// Import common attributes
// include::{docdir}/../snippets/attributes/common.adoc[]
// Custom attributes
:overview: <product-overview>

// Github-Specific Settings
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

[preface]
== Preface

{overview}

== Getting Started

This guide assumes you are familiar with Spark set up and Spark Structured Streaming concepts. In the following sections we will show how to set up Solace Spark Connector to stream data from Solace to Spark.

=== Prerequisites

* https://solace.com/products/event-broker/[Solace PubSub+ Event Broker]
* Apache Spark 3.5.1, Scala 2.12

=== Quick Start common steps

include::{docdir}/../sections/general/quick-start/quick-start.adoc[leveloffset=+2]

NOTE: Above sample code used parquet as example data source. You can configure your required data source to write data.

=== Databricks Considerations

In case if you are using Shared compute cluster, make sure your cluster has https://docs.databricks.com/en/data-governance/unity-catalog/manage-privileges/allowlist.html[appropriate permissions] to install connector from maven central and access the jars. Please contact your Databricks administrator for required permissions.

=== Checkpointing & Acknowledgement

Solace Spark connector relies on Spark Checkpointing mechanism to resume from last successful message. Connector acknowledges & writes processed message id's to spark checkpoint whenever Spark sends commit signal to connector. Commit signal is an indication by Spark that data has been processed successfully.

=== Checkpoint Handling

In some cases, there might be checkpoint failures as spark may fail to write to checkpoint during instance crash or unavailability or other reasons. Though the connector will handle duplicates in most cases, we recommend to keep your downstream systems idempotent.

=== Checkpoint Strategies

Solace Spark connector support storing connector state in Solace Last Value Queue along with Spark Checkpoint. This is enabled by default to support fault tolerance scenarios like environment recovery, disaster recovery or spark checkpoint failures. The connector first checks the state in LVQ and in case of any failures in LVQ look up it moves to Spark Checkpointing.

NOTE: Solace Last Value Queue will only store successfully processed message ids.

NOTE: Duplicates cannot be eliminated completely as checkpointing might fail for both Spark and LVQ when there are unavoidable instances.

=== Connector Recovery from Failures

Solace Spark connector can recover from failures using the state present in checkpoint location. Please refer to <<Checkpoint Strategies>>.

As part of recovery strategy, connector provides the ability for the user to override the default checkpoint strategy by configuring <<lastSuccessfulMessageId, lastSuccessfulMessageId>> property with the successfully processed unique identifier in output data stream.

During recovery, the connector will match the incoming message id with configured id and handle duplicate messages.

The connector applies a <<lastSuccessfulMessageIdThreshold, lastSuccessfulMessageIdThreshold>> value to identify the configured id to handle duplicates. The default threshold value is equal to <<batchSize, batchSize>> property. In case if lastSuccessfulMessageId is not available in the given threshold value the connector will continue to process all messages.

=== User Authentication

Solace Spark Connector supports Basic and OAuth authentication to Solace. Client Credentials flow is supported when connecting using OAuth.

Use below options to enable OAuth authentication. For property description please refer to <<Configuration>> section.

[source,scala]
----
    spark.readStream.format("solace").option("host", "")
    .option("vpn", "default")
    .option("solace.apiProperties.AUTHENTICATION_SCHEME", "AUTHENTICATION_SCHEME_OAUTH2")
    .option("solace.oauth.client.auth-server-url", "")
    .option("solace.oauth.client.client-id", "")
    .option("solace.oauth.client.credentials.client-secret", "")
    .option("solace.oauth.client.auth-server.client-certificate.file", "")
    .option("solace.oauth.client.auth-server.truststore.file", "")
    .option("solace.oauth.client.auth-server.truststore.password", "")
    .option("solace.oauth.client.auth-server.ssl.validate-certificate", false)
    .option("solace.oauth.client.token.refresh.interval", 110)
----

=== Solace Spark Streaming Schema Structure

Solace Spark Connector transforms the incoming message to Spark row with below schema definition.

[cols="2m,2m", options="header"]
|===
| Column Name
| Column Type

| Id
| String

| Payload
| Binary

| PartitionKey
| String

| Topic
| String

| TimeStamp
| Timestamp

| Headers
| Map<string, binary>
|===

== Configuration

include::{docdir}/../sections/general/configuration/spark-config.adoc[leveloffset=+2]

NOTE: This connector is tested on Databricks environment with Cluster Version 14.3 LTS (includes Apache Spark 3.5.0, Scala 2.12)

== License

This project is licensed under the Solace Community License, Version 1.0. - See the `LICENSE` file for details.

include::{docdir}/../sections/general/support.adoc[leveloffset=+2]